{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightly edited for clarity as Noah goes through this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compartmental Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://pyro.ai/examples/sir_hmc.html by Swapneel and Noah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import distributions as tdist\n",
    "from pyro import distributions as dist\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.hmm\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer import MCMC, NUTS, config_enumerate, infer_discrete\n",
    "from pyro.infer.autoguide import init_to_value\n",
    "from pyro.ops.special import safe_log\n",
    "from pyro.ops.tensor_utils import convolve\n",
    "from pyro.util import warn_if_nan\n",
    "#plt.style.use('dark_background')\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to run this. Memory gets used up. Will update as I go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Priors we place on R0 and $\\rho$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "r0_dist = tdist.LogNormal(0., 1.) # we form a prior distribution on r0 that it's a lognormal dist. \n",
    "r0_samples = r0_dist.sample(torch.Size([10000]))\n",
    "\n",
    "ax[0].hist(r0_samples)\n",
    "#sns.hist(r0_samples, ax=ax[0])\n",
    "ax[0].set_xlabel('R0 (reproduction number)')\n",
    "ax[0].set_title('Prior on R0 (reproduction number)')\n",
    "\n",
    "#sns.hist\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_model(population):\n",
    "    \"\"\"\n",
    "    From the population and initial values for recovery time, R0, and rho, convert interpretable values\n",
    "    \n",
    "    (an epidemiologist knows what R0 is but not rate_s), into the parameters needed for the distribution. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    rate_s: rate of infection i \n",
    "    \n",
    "    prob_i: probality of infection? \n",
    "    \n",
    "    rho: not sure yet. \n",
    "    \"\"\"\n",
    "    tau = args.recovery_time  # Assume this can be measured exactly... TODO: Why not make this also a dist? \n",
    "    R0 = pyro.sample(\"R0\", dist.LogNormal(0., 1.)) #these models are priors on what we considered args. \n",
    "    rho = pyro.sample(\"rho\", dist.Uniform(0, 1))\n",
    "\n",
    "    # Convert interpretable parameters to distribution parameters.\n",
    "    rate_s = -R0 / (tau * population)\n",
    "    prob_i = 1 / (1 + tau)\n",
    "\n",
    "    return rate_s, prob_i, rho\n",
    "\n",
    "\n",
    "def discrete_model(args, data):\n",
    "    \"\"\"\n",
    "    create discrete model \n",
    "    \"\"\"\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population)\n",
    "\n",
    "    # Sequentially sample time-local variables.\n",
    "    S = torch.tensor(args.population - 1.)\n",
    "    I = torch.tensor(1.)\n",
    "    for t, datum in enumerate(data): # for data point in dataset, use this distribution to  \n",
    "        S2I = pyro.sample(\"S2I_{}\".format(t),\n",
    "                          dist.Binomial(S, -(rate_s * I).expm1()))\n",
    "        I2R = pyro.sample(\"I2R_{}\".format(t),\n",
    "                          dist.Binomial(I, prob_i))\n",
    "        S = pyro.deterministic(\"S_{}\".format(t), S - S2I) \n",
    "        I = pyro.deterministic(\"I_{}\".format(t), I + S2I - I2R) # the diff eqs\n",
    "        pyro.sample(\"obs_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S2I, rho),\n",
    "                    obs=datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this model to simulate data. We'll use poutine.condition to pin\n",
    "# parameter values and poutine.trace to record sample observations.\n",
    "\n",
    "def generate_data(args):\n",
    "    \"\"\"\n",
    "    Simulate data. \n",
    "    \n",
    "    Using args to input initial estimates on R0 and other epidemiology jargon, return a list of the compartment populations\n",
    "    according to these conditions. \n",
    "    \n",
    "    S2I, I2R\n",
    "    \"\"\"\n",
    "    logging.info(\"Generating data...\")\n",
    "    params = {\"R0\": torch.tensor(args.basic_reproduction_number),\n",
    "              \"rho\": torch.tensor(args.response_rate)}\n",
    "    empty_data = [None] * (args.duration + args.forecast)\n",
    "\n",
    "    # We'll retry until we get an actual outbreak.\n",
    "    for attempt in range(100):\n",
    "        with poutine.trace() as tr:\n",
    "            with poutine.condition(data=params):\n",
    "                discrete_model(args, empty_data)\n",
    "\n",
    "        # Concatenate sequential time series into tensors.\n",
    "        obs = torch.stack([site[\"value\"]\n",
    "                           for name, site in tr.trace.nodes.items()\n",
    "                           if re.match(\"obs_[0-9]+\", name)])\n",
    "        S2I = torch.stack([site[\"value\"]\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if re.match(\"S2I_[0-9]+\", name)])\n",
    "    \n",
    "        I2R = torch.stack([site[\"value\"]\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if re.match(\"I2R_[0-9]+\", name)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        assert len(obs) == len(empty_data)\n",
    "\n",
    "        obs_sum = int(obs[:args.duration].sum())\n",
    "        S2I_sum = int(S2I[:args.duration].sum())\n",
    "        I2R_sum = int(I2R[:args.duration].sum())\n",
    "        if obs_sum >= args.min_observations:\n",
    "            logging.info(\"Observed {:d}/{:d} infections:\\n{}\".format(\n",
    "                obs_sum, S2I_sum, I2R_sum, \" \".join([str(int(x)) for x in obs[:args.duration]])))\n",
    "            return {\"S2I\": S2I, \"obs\": obs, \"I2R\": I2R}\n",
    "\n",
    "    raise ValueError(\"Failed to generate {} observations. Try increasing \"\n",
    "                     \"--population or decreasing --min-observations\"\n",
    "                     .format(args.min_observations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(population=1000, \n",
    "                 min_observations=3, \n",
    "                 duration=100, \n",
    "                 forecast=0, \n",
    "                 basic_reproduction_number=5.5,  # what our epidemic's R0 is.\n",
    "                 recovery_time=7.0,  #recovery timne. These values are \n",
    "                 response_rate=0.5, \n",
    "                 enum=True, \n",
    "                 sequential=True, \n",
    "                 num_samples=2000, \n",
    "                 warmup_steps=100, \n",
    "                 max_tree_depth=5, \n",
    "                 rng_seed=0, \n",
    "                 double=True, \n",
    "                 jit=True, \n",
    "                 cuda=True, \n",
    "                 verbose=True, \n",
    "                 plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Observed 493/998 infections:\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'S2I': tensor([  0.,   0.,   0.,   1.,   1.,   2.,   3.,   7.,   9.,  11.,  20.,  26.,\n",
       "          41.,  60.,  96., 111., 131., 138., 101.,  76.,  67.,  34.,  19.,  17.,\n",
       "          13.,   5.,   3.,   1.,   0.,   1.,   1.,   0.,   1.,   1.,   1.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.]),\n",
       " 'obs': tensor([ 0.,  0.,  0.,  0.,  0.,  2.,  2.,  0.,  3.,  6., 14., 13., 20., 32.,\n",
       "         49., 59., 60., 65., 58., 35., 35., 18.,  6.,  8.,  5.,  1.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.]),\n",
       " 'I2R': tensor([ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  4.,  4.,  3., 12., 11.,\n",
       "         29., 32., 35., 43., 44., 60., 75., 74., 67., 52., 47., 52., 48., 28.,\n",
       "         45., 35., 23., 28., 21., 12., 16., 12., 10.,  8.,  7.,  6.,  9.,  5.,\n",
       "         10.,  2.,  9.,  2.,  3.,  2.,  2.,  0.,  2.,  0.,  2.,  1.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = generate_data(args)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x['S2I'],label='Susceptible to Infected')\n",
    "plt.plot(x['I2R'],label='Infected to Recovered')\n",
    "plt.plot(x['obs'],label='observed data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# =========\n",
    "#\n",
    "# While the above discrete_model is easy to understand, its discrete latent\n",
    "# variables pose a challenge for inference. One of the most popular inference\n",
    "# strategies for such models is Sequential Monte Carlo. However since Pyro and\n",
    "# PyTorch are stronger in gradient based vectorizable inference algorithms, we\n",
    "# will instead pursue inference based on Hamiltonian Monte Carlo (HMC).\n",
    "#\n",
    "# Our general inference strategy will be to:\n",
    "# 1. Introduce auxiliary variables to make the model Markov.\n",
    "# 2. Introduce more auxiliary variables to create a discrete parameterization.\n",
    "# 3. Marginalize out all remaining discrete latent variables.\n",
    "# 4. Vectorize to enable parallel-scan temporal filtering.\n",
    "#\n",
    "# Let's consider reparameterizing in terms of the variables (S, I) rather than\n",
    "# (S2I, I2R). Since these may lead to inconsistent states, we need to replace\n",
    "# the Binomial transition factors (S2I, I2R) with ExtendedBinomial.\n",
    "#\n",
    "# The following model is equivalent to the discrete_model:\n",
    "\n",
    "@config_enumerate\n",
    "def reparameterized_discrete_model(args, data):\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population)\n",
    "\n",
    "    # Sequentially sample time-local variables.\n",
    "    S_curr = torch.tensor(args.population - 1.)\n",
    "    I_curr = torch.tensor(1.)\n",
    "    for t, datum in enumerate(data):\n",
    "        # Sample reparameterizing variables.\n",
    "        # When reparameterizing to a factor graph, we ignored density via\n",
    "        # .mask(False). Thus distributions are used only for initialization.\n",
    "        S_prev, I_prev = S_curr, I_curr\n",
    "        S_curr = pyro.sample(\"S_{}\".format(t),\n",
    "                             dist.Binomial(args.population, 0.5).mask(False))\n",
    "        I_curr = pyro.sample(\"I_{}\".format(t),\n",
    "                             dist.Binomial(args.population, 0.5).mask(False))\n",
    "\n",
    "        # Now we reverse the computation.\n",
    "        S2I = S_prev - S_curr\n",
    "        I2R = I_prev - I_curr + S2I\n",
    "        pyro.sample(\"S2I_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()),\n",
    "                    obs=S2I)\n",
    "        pyro.sample(\"I2R_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(I_prev, prob_i),\n",
    "                    obs=I2R)\n",
    "        pyro.sample(\"obs_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S2I, rho),\n",
    "                    obs=datum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# By reparameterizing, we have converted to coordinates that make the model\n",
    "# Markov. We have also replaced dynamic integer_interval constraints with\n",
    "# easier static integer_interval constraints (although we'll still need good\n",
    "# initialization to avoid NANs). Since the discrete latent variables are\n",
    "# bounded (by population size), we can enumerate out discrete latent variables\n",
    "# and perform HMC inference over the global latents. However enumeration\n",
    "# complexity is O(population^4), so this is only feasible for very small\n",
    "# populations.\n",
    "#\n",
    "# Here is an inference approach using an MCMC sampler.\n",
    "\n",
    "def infer_hmc_enum(args, data):\n",
    "    model = reparameterized_discrete_model\n",
    "    return _infer_hmc(args, data, model)\n",
    "\n",
    "\n",
    "\n",
    "def _infer_hmc(args, data, model, init_values={}):\n",
    "    \"\"\"\n",
    "    Run Hamiltonian Monte Carlo mcmc to estimate parameters R0 and rho\n",
    "    \"\"\"\n",
    "    logging.info(\"Running inference...\")\n",
    "    kernel = NUTS(model,\n",
    "                  full_mass=[(\"R0\", \"rho\")],\n",
    "                  max_tree_depth=args.max_tree_depth,\n",
    "                  init_strategy=init_to_value(values=init_values),\n",
    "                  jit_compile=args.jit, ignore_jit_warnings=True)\n",
    "\n",
    "    # We'll define a hook_fn to log potential energy values during inference.\n",
    "    # This is helpful to diagnose whether the chain is mixing.\n",
    "    energies = []\n",
    "\n",
    "    def hook_fn(kernel, *unused):\n",
    "        e = float(kernel._potential_energy_last)\n",
    "        energies.append(e)\n",
    "        if args.verbose:\n",
    "            logging.info(\"potential = {:0.6g}\".format(e))\n",
    "\n",
    "    mcmc = MCMC(kernel, hook_fn=hook_fn,\n",
    "                num_samples=args.num_samples,\n",
    "                warmup_steps=args.warmup_steps)\n",
    "    mcmc.run(args, data)\n",
    "    mcmc.summary()\n",
    "    if args.plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(energies)\n",
    "        plt.xlabel(\"MCMC step\")\n",
    "        plt.ylabel(\"potential energy\")\n",
    "        plt.title(\"MCMC energy trace\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    samples = mcmc.get_samples()\n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scale to large populations, we'll continue to reparameterize, this time\n",
    "# replacing each of (S_aux,I_aux) with a combination of a bounded real\n",
    "# variable and a Categorical variable with only four values.\n",
    "#\n",
    "# This is the crux: we can now perform HMC over the real variable and\n",
    "# marginalize out the Categorical variables using variable elimination.\n",
    "#\n",
    "# We first define a helper to create enumerated Categorical sites.\n",
    "\n",
    "def quantize(name, x_real, min, max):\n",
    "    \"\"\"\n",
    "    Randomly quantize in a way that preserves probability mass.\n",
    "    We use a piecewise polynomial spline of order 3.\n",
    "    \"\"\"\n",
    "    assert min < max\n",
    "    lb = x_real.detach().floor()\n",
    "\n",
    "    # This cubic spline interpolates over the nearest four integers, ensuring\n",
    "    # piecewise quadratic gradients.\n",
    "    s = x_real - lb\n",
    "    ss = s * s\n",
    "    t = 1 - s\n",
    "    tt = t * t\n",
    "    probs = torch.stack([\n",
    "        t * tt,\n",
    "        4 + ss * (3 * s - 6),\n",
    "        4 + tt * (3 * t - 6),\n",
    "        s * ss,\n",
    "    ], dim=-1) * (1/6)\n",
    "    q = pyro.sample(\"Q_\" + name, dist.Categorical(probs)).type_as(x_real)\n",
    "\n",
    "    x = lb + q - 1\n",
    "    x = torch.max(x, 2 * min - 1 - x)\n",
    "    x = torch.min(x, 2 * max + 1 - x)\n",
    "\n",
    "    return pyro.deterministic(name, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define another equivalent model.\n",
    "\n",
    "@config_enumerate\n",
    "def continuous_model(args, data):\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population)\n",
    "\n",
    "    # Sample reparameterizing variables.\n",
    "    S_aux = pyro.sample(\"S_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "    I_aux = pyro.sample(\"I_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "\n",
    "    # Sequentially sample time-local variables.\n",
    "    S_curr = torch.tensor(args.population - 1.)\n",
    "    I_curr = torch.tensor(1.)\n",
    "    for t, datum in poutine.markov(enumerate(data)):\n",
    "        S_prev, I_prev = S_curr, I_curr\n",
    "        S_curr = quantize(\"S_{}\".format(t), S_aux[..., t], min=0, max=args.population)\n",
    "        I_curr = quantize(\"I_{}\".format(t), I_aux[..., t], min=0, max=args.population)\n",
    "\n",
    "        # Now we reverse the computation.\n",
    "        S2I = S_prev - S_curr\n",
    "        I2R = I_prev - I_curr + S2I\n",
    "        pyro.sample(\"S2I_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()),\n",
    "                    obs=S2I)\n",
    "        pyro.sample(\"I2R_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(I_prev, prob_i),\n",
    "                    obs=I2R)\n",
    "        pyro.sample(\"obs_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S2I, rho),\n",
    "                    obs=datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now all latent variables in the continuous_model are either continuous or\n",
    "# enumerated, so we can use HMC. However we need to take special care with\n",
    "# constraints because the above Markov reparameterization covers regions of\n",
    "# hypothesis space that are infeasible (i.e. whose log_prob is -infinity). We\n",
    "# thus heuristically initialize to a feasible point.\n",
    "\n",
    "def heuristic_init(args, data):\n",
    "    \"\"\"Heuristically initialize to a feasible point.\"\"\"\n",
    "    # Start with a single infection.\n",
    "    S0 = args.population - 1\n",
    "    # Assume 50% <= response rate <= 100%.\n",
    "    S2I = data * min(2., (S0 / data.sum()).sqrt())\n",
    "    S_aux = (S0 - S2I.cumsum(-1)).clamp(min=0.5) # clamp \n",
    "    # Account for the single initial infection.\n",
    "    S2I[0] += 1\n",
    "    # Assume infection lasts less than a month.\n",
    "    recovery = torch.arange(30.).div(args.recovery_time).neg().exp()\n",
    "    I_aux = convolve(S2I, recovery)[:len(data)].clamp(min=0.5)\n",
    "\n",
    "    return {\n",
    "        \"R0\": torch.tensor(2.0),\n",
    "        \"rho\": torch.tensor(0.5),\n",
    "        \"S_aux\": S_aux,\n",
    "        \"I_aux\": I_aux,\n",
    "    }\n",
    "\n",
    "\n",
    "def infer_hmc_cont(model, args, data):\n",
    "    init_values = heuristic_init(args, data)\n",
    "    return _infer_hmc(args, data, model, init_values=init_values)\n",
    "\n",
    "\n",
    "# Our final inference trick is to vectorize. We can repurpose DiscreteHMM's\n",
    "# implementation here, but we'll need to manually represent a Markov\n",
    "# neighborhood of multiple Categorical of size 4 as single joint Categorical\n",
    "# with 4 * 4 = 16 states, and then manually perform variable elimination (the\n",
    "# factors here don't quite conform to DiscreteHMM's interface).\n",
    "\n",
    "def quantize_enumerate(x_real, min, max):\n",
    "    \"\"\"\n",
    "    Randomly quantize in a way that preserves probability mass.\n",
    "    We use a piecewise polynomial spline of order 3.\n",
    "    \"\"\"\n",
    "    assert min < max\n",
    "    lb = x_real.detach().floor()\n",
    "\n",
    "    # This cubic spline interpolates over the nearest four integers, ensuring\n",
    "    # piecewise quadratic gradients.\n",
    "    s = x_real - lb\n",
    "    ss = s * s\n",
    "    t = 1 - s\n",
    "    tt = t * t\n",
    "    probs = torch.stack([\n",
    "        t * tt,\n",
    "        4 + ss * (3 * s - 6),\n",
    "        4 + tt * (3 * t - 6),\n",
    "        s * ss,\n",
    "    ], dim=-1) * (1/6)\n",
    "    logits = safe_log(probs)\n",
    "    q = torch.arange(-1., 3.)\n",
    "\n",
    "    x = lb.unsqueeze(-1) + q\n",
    "    x = torch.max(x, 2 * min - 1 - x)\n",
    "    x = torch.min(x, 2 * max + 1 - x)\n",
    "    return x, logits\n",
    "\n",
    "\n",
    "def vectorized_model(args, data):\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population)\n",
    "\n",
    "    # Sample reparameterizing variables.\n",
    "    S_aux = pyro.sample(\"S_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "    I_aux = pyro.sample(\"I_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "\n",
    "    # Manually enumerate.\n",
    "    S_curr, S_logp = quantize_enumerate(S_aux, min=0, max=args.population)\n",
    "    I_curr, I_logp = quantize_enumerate(I_aux, min=0, max=args.population)\n",
    "    # Truncate final value from the right then pad initial value onto the left.\n",
    "    S_prev = torch.nn.functional.pad(S_curr[:-1], (0, 0, 1, 0), value=args.population - 1)\n",
    "    I_prev = torch.nn.functional.pad(I_curr[:-1], (0, 0, 1, 0), value=1)\n",
    "    # Reshape to support broadcasting, similar to EnumMessenger.\n",
    "    T = len(data)\n",
    "    Q = 4\n",
    "    S_prev = S_prev.reshape(T, Q, 1, 1, 1)\n",
    "    I_prev = I_prev.reshape(T, 1, Q, 1, 1)\n",
    "    S_curr = S_curr.reshape(T, 1, 1, Q, 1)\n",
    "    S_logp = S_logp.reshape(T, 1, 1, Q, 1)\n",
    "    I_curr = I_curr.reshape(T, 1, 1, 1, Q)\n",
    "    I_logp = I_logp.reshape(T, 1, 1, 1, Q)\n",
    "    data = data.reshape(T, 1, 1, 1, 1)\n",
    "\n",
    "    # Reverse the S2I,I2R computation.\n",
    "    S2I = S_prev - S_curr\n",
    "    I2R = I_prev - I_curr + S2I\n",
    "\n",
    "    # Compute probability factors.\n",
    "    S2I_logp = dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()).log_prob(S2I)\n",
    "    I2R_logp = dist.ExtendedBinomial(I_prev, prob_i).log_prob(I2R)\n",
    "    obs_logp = dist.ExtendedBinomial(S2I, rho).log_prob(data)\n",
    "\n",
    "    # Manually perform variable elimination.\n",
    "    logp = S_logp + (I_logp + obs_logp) + S2I_logp + I2R_logp\n",
    "    logp = logp.reshape(-1, Q * Q, Q * Q)\n",
    "    logp = pyro.distributions.hmm._sequential_logmatmulexp(logp)\n",
    "    logp = logp.reshape(-1).logsumexp(0)\n",
    "    logp = logp - math.log(4)  # Account for S,I initial distributions.\n",
    "    warn_if_nan(logp)\n",
    "    pyro.factor(\"obs\", logp)\n",
    "\n",
    "\n",
    "# We can fit vectorized_model exactly as we fit the original continuous_model,\n",
    "# using our infer_hmc_cont helper. The vectorized model is more than an order\n",
    "# of magnitude faster than the sequential version, and scales logarithmically\n",
    "# in time (up to your machine's parallelism).\n",
    "#\n",
    "# After inference we have samples of all latent variables. Let's define a\n",
    "# helper to examine the inferred posterior distributions.\n",
    "\n",
    "def evaluate(args, samples):\n",
    "    # Print estimated values.\n",
    "    names = {\"basic_reproduction_number\": \"R0\",\n",
    "             \"response_rate\": \"rho\"}\n",
    "    for name, key in names.items():\n",
    "        mean = samples[key].mean().item()\n",
    "        std = samples[key].std().item()\n",
    "        logging.info(\"{}: truth = {:0.3g}, estimate = {:0.3g} \\u00B1 {:0.3g}\"\n",
    "                     .format(key, getattr(args, name), mean, std))\n",
    "\n",
    "    # Optionally plot histograms.\n",
    "    if args.plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(5, 5))\n",
    "        axes[0].set_title(\"Posterior parameter estimates\")\n",
    "        for ax, (name, key) in zip(axes, names.items()):\n",
    "            truth = getattr(args, name)\n",
    "            sns.distplot(samples[key], ax=ax, label=\"posterior\")\n",
    "            ax.axvline(truth, color=\"k\", label=\"truth\")\n",
    "            ax.set_xlabel(key + \" = \" + name.replace(\"_\", \" \"))\n",
    "            ax.set_yticks(())\n",
    "            ax.legend(loc=\"best\")\n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction and Forecasting\n",
    "# ==========================\n",
    "#\n",
    "# So far we've written four models that each describe the same probability\n",
    "# distribution. Each successive model made inference cheaper. Next let's move\n",
    "# beyond inference and consider predicting latent infection rate and\n",
    "# forecasting future infections.\n",
    "#\n",
    "# We'll use Pyro's effect handlers to combine multiple of the above models,\n",
    "# leveraging the vectorized_model for inference, then the continuous_model to\n",
    "# compute local latent variables, and finally the original discrete_model to\n",
    "# forecast forward in time. Let's assume posterior samples have already been\n",
    "# generated via infer_hmc_cont(vectorized_model, ...).\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(args, data, samples, truth=None):\n",
    "    logging.info(\"Forecasting {} steps ahead...\".format(args.forecast))\n",
    "    particle_plate = pyro.plate(\"particles\", args.num_samples, dim=-1)\n",
    "\n",
    "    # First we sample discrete auxiliary variables from the continuous\n",
    "    # variables sampled in vectorized_model. This samples only time steps\n",
    "    # [0:duration]. Here infer_discrete runs a forward-filter backward-sample\n",
    "    # algorithm. We'll add these new samples to the existing dict of samples.\n",
    "    model = poutine.condition(continuous_model, samples)\n",
    "    model = particle_plate(model)\n",
    "    model = infer_discrete(model, first_available_dim=-2)\n",
    "    with poutine.trace() as tr:\n",
    "        model(args, data)\n",
    "    samples = OrderedDict((name, site[\"value\"])\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if site[\"type\"] == \"sample\")\n",
    "\n",
    "    # Next we'll run the forward generative process in discrete_model. This\n",
    "    # samples time steps [duration:duration+forecast]. Again we'll update the\n",
    "    # dict of samples.\n",
    "    extended_data = list(data) + [None] * args.forecast\n",
    "    model = poutine.condition(discrete_model, samples)\n",
    "    model = particle_plate(model)\n",
    "    with poutine.trace() as tr:\n",
    "        model(args, extended_data)\n",
    "    samples = OrderedDict((name, site[\"value\"])\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if site[\"type\"] == \"sample\")\n",
    "\n",
    "    # Finally we'll concatenate the sequentially sampled values into contiguous\n",
    "    # tensors. This operates on the entire time interval [0:duration+forecast].\n",
    "    for key in (\"S\", \"I\", \"S2I\", \"I2R\"):\n",
    "        pattern = key + \"_[0-9]+\"\n",
    "        series = [value\n",
    "                  for name, value in samples.items()\n",
    "                  if re.match(pattern, name)]\n",
    "        assert len(series) == args.duration + args.forecast\n",
    "        series[0] = series[0].expand(series[1].shape)\n",
    "        samples[key] = torch.stack(series, dim=-1)\n",
    "    S2I = samples[\"S2I\"]\n",
    "    median = S2I.median(dim=0).values\n",
    "    logging.info(\"Median prediction of new infections (starting on day 0):\\n{}\"\n",
    "                 .format(\" \".join(map(str, map(int, median)))))\n",
    "\n",
    "    # Optionally plot the latent and forecasted series of new infections.\n",
    "    if args.plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        time = torch.arange(args.duration + args.forecast)\n",
    "        p05 = S2I.kthvalue(int(round(0.5 + 0.05 * args.num_samples)), dim=0).values\n",
    "        p95 = S2I.kthvalue(int(round(0.5 + 0.95 * args.num_samples)), dim=0).values\n",
    "        plt.fill_between(time, p05, p95, color=\"red\", alpha=0.3, label=\"90% CI\")\n",
    "        plt.plot(time, median, \"r-\", label=\"median\")\n",
    "        plt.plot(time[:args.duration], data, \"k.\", label=\"observed\")\n",
    "        if truth is not None:\n",
    "            plt.plot(time, truth, \"k--\", label=\"truth\")\n",
    "        plt.axvline(args.duration - 0.5, color=\"gray\", lw=1)\n",
    "        plt.xlim(0, len(time) - 1)\n",
    "        plt.ylim(0, None)\n",
    "        plt.xlabel(\"day after first infection\")\n",
    "        plt.ylabel(\"new infections per day\")\n",
    "        plt.title(\"New infections in population of {}\".format(args.population))\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
