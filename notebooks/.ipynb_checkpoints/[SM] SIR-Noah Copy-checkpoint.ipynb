{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightly edited for clarity as Noah goes through this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Discuss: \n",
    "When we combine the code, maybe we should not introduce u straight up but after we demonstrate the basic ideas underlying our SIR model. In this notebook I basically modified the base model with the policy intervention term `u`, but in Noah's version of the same notebook there will still be a basic example without `u`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disease modeling can be done at the population-level by assuming a global mathematical model that abstracts away individual level effects. A compartmental model is one such model that divides the population of individuals into different states or compartments at each point in time. The general idea is that individuals in a population are interacting with each other and as a result of this interaction they may transition from one compartment to another with a particular transition probability. Since all individuals are assumed to be equal, this transition need only be represented by a global transition probability between every set of subsequent compartments which encapsulates the notion of 'spread' of disease through a population through individual-level compartmental transitions. It helps that in practice, in addition to the number of deaths, the fraction of total population in the infected state/compartment at a particular time is also what we typically utilise in public awareness campaigns, which can be obtained directly from a compartmental model. \n",
    "\n",
    "We will start by looking at a simple compartmental model and move to more complicated models that better capture population interaction and the progression of disease spread. Unless specified, our explanations will reference discrete time steps although as the plots can confirm, this is easily understood in terms of a continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SIR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Dynamics\n",
    "\n",
    "- We work with the assumption that the total population is constant, ignoring new births and deaths for the time being, in this simplistic setup for epidemiological modeling of COVID-19.\n",
    "- write down the differential equations for SIR\n",
    "- explain why the rate of change of the susceptible population S at time t depends on the fraction of infected population at that time i(t) (hint: number of infectious contacts relies on the fraction i(t) of infected people in the population times the total number of contacts times the transmission probability--defined as the probability that an infectious contact may result in a spread of the disease).\n",
    "- the transition from state I to R is fairly straightforward and obtained directly from the rate of recovery of the infected population.\n",
    "- Consider model parameters $\\beta$ and $\\gamma$ where $\\beta$ represents the transmission probability of the disease and $\\gamma$ represents the recovery rate. The model infectiousness or reproduction number $R_0$ is defined as $\\frac{\\beta}{\\gamma}$.\n",
    "- These dynamics are encapsulated in the following differential equations representing the mathematical model for SIR.\n",
    "- Note that we should formally only use S(t), I(t) and R(t) but we sometimes omit it for simplicity (right?)\n",
    "$$\n",
    "s(t) + i(t) + r(t) = 1 \\\\\n",
    "\\frac{dS}{dt} = -\\beta S(t) i(t) \\\\\n",
    "\\frac{dI}{dt} = \\beta S(t) i(t) - \\gamma I(t) \\\\\n",
    "\\frac{dR}{dt} = \\gamma I(t)\n",
    "$$\n",
    "\n",
    "Worth reading [this for a better example of solving these systems of equations mathematically](http://sherrytowers.com/2013/12/11/introduction-to-compartmental-modeling/) plus the three links at the bottom of her page describing interesting extensions to vanilla SIR models through sub-grouping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://pyro.ai/examples/sir_hmc.html by Swapneel and Noah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import distributions as tdist\n",
    "from pyro import distributions as dist\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.hmm\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer import MCMC, NUTS, config_enumerate, infer_discrete\n",
    "from pyro.infer.autoguide import init_to_value\n",
    "from pyro.ops.special import safe_log\n",
    "from pyro.ops.tensor_utils import convolve\n",
    "from pyro.util import warn_if_nan\n",
    "#plt.style.use('dark_background')\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to run this. Memory gets used up. Will update as I go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Priors we place on R0 and $\\rho$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "r0_prior = tdist.LogNormal(0., 1.)\n",
    "r0_samples = r0_prior.sample(torch.Size([100]))\n",
    "sns.histplot(r0_samples[r0_samples < 10].exp(), ax=ax[0])\n",
    "ax[0].set_xlabel('R0 (reproduction number)')\n",
    "ax[0].set_title('Prior on R0 (reproduction number)')\n",
    "\n",
    "rho_prior = tdist.Uniform(0., 1.)\n",
    "rho_samples = rho_prior.sample(torch.Size([10000]))\n",
    "sns.histplot(rho_samples, ax=ax[1])\n",
    "ax[1].set_xlabel('Rho (transmission probability?)')\n",
    "ax[1].set_title('Prior on Rho')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_model(population):\n",
    "    \"\"\"\n",
    "    From the population and initial values for recovery time, R0, and rho, convert interpretable values\n",
    "    \n",
    "    (an epidemiologist knows what R0 is but not rate_s), into the parameters needed for the distribution. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    rate_s: rate of infection i \n",
    "    \n",
    "    prob_i: probality of infection? \n",
    "    \n",
    "    rho: not sure yet. \n",
    "    \n",
    "    u: effect of policy intervention\n",
    "    \"\"\"\n",
    "    tau = args.recovery_time  # Assume this can be measured exactly... TODO: Why not make this also a dist? \n",
    "    R0 = pyro.sample(\"R0\", dist.LogNormal(0., 1.)) # these are priors on what we considered args. \n",
    "    rho = pyro.sample(\"rho\", dist.Uniform(0, 1))\n",
    "    \n",
    "    # Convert interpretable parameters to distribution parameters.\n",
    "    # rate_s = - R0 / (tau * population)  # I don't see the intuition behind this because for larger populations this foews close to 0\n",
    "    rate_s = - R0 / (tau * population)\n",
    "    prob_i = 1 / (1 + tau)\n",
    "\n",
    "    return rate_s, prob_i, rho\n",
    "\n",
    "\n",
    "def discrete_model(args, data):\n",
    "    \"\"\"\n",
    "    create discrete model \n",
    "    \n",
    "    This enumerates through the data, and computes what the actual S and I (and therefore R) values R at each timestep, \n",
    "    and saves in a messy dictionary under poutine. \n",
    "    \n",
    "    This messy info is then saved in a more ordered format through \n",
    "    \"\"\"\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population) # global parameters are what the current estimates are. \n",
    "    \n",
    "    # Introduce policy intervention parameter 'u' that limits the rate of spread of disease\n",
    "    # This can technically just be uniformly drawn from (0, 1) but it is more realistic \n",
    "    # to limit it to less than 1 because no intervention can be perfect enough to\n",
    "    # completely curtail a disease.\n",
    "    if args.policy_intervention > 0:\n",
    "        # u = pyro.param(\"u\", torch.tensor([args.policy_intervention]))\n",
    "        u = pyro.sample(\"u\", dist.Normal(args.policy_intervention, 0.05))\n",
    "        rate_s = (1 - u) * rate_s\n",
    "        \n",
    "    # Sequentially sample time-local variables.\n",
    "    S = torch.tensor(args.population - 1.)\n",
    "    I = torch.tensor(1.)\n",
    "    for t, datum in enumerate(data): # for data point in dataset, use this distribution to  \n",
    "        S2I = pyro.sample(\"S2I_{}\".format(t),\n",
    "                          dist.Binomial(S, -(rate_s * I).expm1())) # the actual formula \n",
    "        I2R = pyro.sample(\"I2R_{}\".format(t),\n",
    "                          dist.Binomial(I, prob_i))\n",
    "        S = pyro.deterministic(\"S_{}\".format(t), S - S2I) \n",
    "        I = pyro.deterministic(\"I_{}\".format(t), I + S2I - I2R) # the diff eqs \n",
    "        pyro.sample(\"obs_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S2I, rho),\n",
    "                    obs=datum)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SIR model above, the authors redefine the population-level disease parameters we discussed for the SIR model (transmission probability $\\beta$ and recovery rate $\\gamma$) in terms of certain measurable quantities such as the recovery time $\\tau$ and the initial estimates of the reproduction number $R_0$ and response rate $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial versus ExtendedBinomial (pyro, experimental). The idea behind the last line in the discrete_model defined below is that after calculating the S2I transitions we sample from a binomial distribution that has a support over the space of integers with a probability $\\rho$ so let's first see how that looks. In the diagram below you see that rho is essentially equivalent to sampling from a binomial distribution that with a certain probability *could* result in such a count of S2I transitions. Hopefully this offers some intuition into what the response rate $\\rho$ represents. While this model does not follow an identical transition process from $S$ to $I$ as described in the SIR model dynamics, $\\rho$ operates in a manner similar to the transition probability $\\beta$ in that it determines how likely it is that the model will generate a particular number of successful infections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize=(9,12))\n",
    "dummyS2I = torch.arange(10, 100, 10)\n",
    "dummyrho = torch.arange(0.1, 0.99, 0.09)\n",
    "count = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        dummydist = dist.ExtendedBinomial(dummyS2I[count], dummyrho[count])\n",
    "        dummysamples = dummydist.sample(torch.Size([10000]))\n",
    "        sns.histplot(dummysamples, ax=ax[i, j])\n",
    "        ax[i, j].set_xlabel('Samples')\n",
    "        ax[i, j].set_title('Frequency')\n",
    "        count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this model to simulate data. We'll use poutine.condition to pin\n",
    "# parameter values and poutine.trace to record sample observations.\n",
    "\n",
    "def generate_data(args):\n",
    "    \"\"\"\n",
    "    Simulate data. \n",
    "    \n",
    "    Using args to input initial estimates on R0 and other epidemiology jargon, return a list of the compartment populations\n",
    "    according to these conditions. \n",
    "    \n",
    "    Policy interventions have been simulated using the parameter 'u' based on the paper by F. Wood and team. \n",
    "    We add a max_observations argument so that we can simulate data with an upper limit for number of infections with the same\n",
    "    values of R0 and rho as we did in the case with no policy intervention.\n",
    "    \n",
    "    S2I, I2R\n",
    "    \"\"\"\n",
    "    logging.info(\"Generating data...\")\n",
    "    params = {\"R0\": torch.tensor(args.basic_reproduction_number), #what we set \n",
    "              \"rho\": torch.tensor(args.response_rate)}\n",
    "    empty_data = [None] * (args.duration + args.forecast)\n",
    "\n",
    "    # We'll retry until we get an actual outbreak -> this implies at least args.min_observations infections are observed\n",
    "    for attempt in range(100):\n",
    "        with poutine.trace() as tr:\n",
    "            with poutine.condition(data=params):\n",
    "                discrete_model(args, empty_data)\n",
    "\n",
    "        # Concatenate sequential time series into tensors.\n",
    "        obs = torch.stack([site[\"value\"]\n",
    "                           for name, site in tr.trace.nodes.items()\n",
    "                           if re.match(\"obs_[0-9]+\", name)])\n",
    "        S2I = torch.stack([site[\"value\"]\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if re.match(\"S2I_[0-9]+\", name)])\n",
    "    \n",
    "        I2R = torch.stack([site[\"value\"]\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if re.match(\"I2R_[0-9]+\", name)])\n",
    "        \n",
    "        assert len(obs) == len(empty_data)\n",
    "\n",
    "        obs_sum = int(obs[:args.duration].sum())\n",
    "        S2I_sum = int(S2I[:args.duration].sum())\n",
    "        I2R_sum = int(I2R[:args.duration].sum())\n",
    "\n",
    "        # we only need max observations if we do not use u, \n",
    "        # or if we want to generate data without using u\n",
    "#         if args.max_observations > 0:\n",
    "#             if obs_sum <= args.max_observations and obs_sum >= args.min_observations:\n",
    "#                 logging.info(\"Observed {:d}/{:d} infections:\\n{}\".format(\n",
    "#                     obs_sum, S2I_sum, I2R_sum, \" \".join([str(int(x)) for x in obs[:args.duration]])))\n",
    "#                 return {\"S2I\": S2I, \"obs\": obs, \"I2R\": I2R}\n",
    "\n",
    "#         else:\n",
    "        if obs_sum >= args.min_observations:\n",
    "            logging.info(\"Observed {:d}/{:d} infections: \\n{}\".format(\n",
    "                obs_sum, S2I_sum, I2R_sum, \" \".join([str(int(x)) for x in obs[:args.duration]])))\n",
    "            return {\"S2I\": S2I, \"obs\": obs, \"I2R\": I2R}\n",
    "\n",
    "    raise ValueError(\"Failed to generate min: {} and max: {} observations. Try increasing \"\n",
    "                     \"--population, increasing --max-observations or decreasing --min-observations\"\n",
    "                     .format(args.min_observations, args.max_observations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(population=1000, \n",
    "                 min_observations=10,\n",
    "                 max_observations=800,\n",
    "                 duration=180,\n",
    "                 forecast=25, \n",
    "                 basic_reproduction_number=1.5,  # what our epidemic's R0 is.\n",
    "                 recovery_time=7.0,  #recovery timne. These values are \n",
    "                 response_rate=0.8,  # this is the probability of observed infections (noisy)\n",
    "                 enum=True, \n",
    "                 sequential=True, \n",
    "                 num_samples=1000, \n",
    "                 warmup_steps=100, \n",
    "                 max_tree_depth=5, \n",
    "                 rng_seed=0, \n",
    "                 double=True, \n",
    "                 jit=True, \n",
    "                 cuda=True, \n",
    "                 verbose=True, \n",
    "                 plot=True,\n",
    "                 policy_intervention=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation Process\n",
    "\n",
    "The way pyro performs inference over discrete latent variables is through enumeration. It defines a set of effect handlers under the hood in order to freely allow new inference algorithms to be developed without having to dig into its internal implementations. Poutine is a library of composable effect handlers that can be used to develop such algorithms. For our use-case, poutine can be used to condition a model on some observable data. But in addition to that, imagine using poputine to run the model forward **without conditioning on any observed data** just to use it as a forward simulator. That is exactly what this data generating process is doing. \n",
    "\n",
    "The generate_data function conditions the model on an \"observed dataset\" which is essentially a list of null values and in doing so it allows the model to run unconditionally to generate a sample time-series of infection spread based on the model parameters (here, $R_0$ and $\\rho$). Then, it checks if the generated time-series satisfies a certain constraint based on which it either accepts or rejects the entire simulated trajectory. It does this by going back and checking the 'trace' produced by the forward simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation-wise, it runs a simulation for `args.duration` timesteps and discards simulated trajectories until it satisfies a constraint of total infections being at least `args.min_observations` for a population of `args.population` individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Generated Infection Spread\n",
    "\n",
    "So far we have defined our model in terms of transitions `S2I` and `I2R` and we only assume that we noisily observe the number of infections based on the response rate $\\rho$. The plot below shows these values with the corresponding axes as labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10, 6))\n",
    "plt.plot(x['S2I'],label='Susceptible to Infected (S2I)')\n",
    "plt.plot(x['I2R'],label='Infected to Recovered (I2R)')\n",
    "plt.plot(x['obs'],'o',label='Observed data')\n",
    "plt.xlabel('Time (Days)')\n",
    "plt.ylabel('No. of Individuals')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the figure does tell us about the true and observed spread, it would be much easier to visualize this if we could model the individual compartments over time using the sum total of individuals belonging to each compartment at each discrete time-step and interpolating between them for ease of visualization. Furthermore, when performing inference, it is harder to marginalize by enumerating these discrete random variables. This is where we can introduce a reparametrization of the original model in terms of $S$, $I$, and implicitly, $R$ using the idea that $S + I + R$ = total population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Varying Policy Interventions (u) on SIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(15,15))\n",
    "dummyu = np.arange(0, 0.7, 0.07)\n",
    "count = 0\n",
    "for ii in range(3):   # since i is used for the compartmental model\n",
    "    for j in range(3):\n",
    "        args.policy_intervention = dummyu[count]\n",
    "        if args.policy_intervention > 0.8:\n",
    "            args.min_observations = 0\n",
    "\n",
    "        torch.manual_seed(1234)\n",
    "        pyro.set_rng_seed(1234)\n",
    "        \n",
    "        try:\n",
    "            data = generate_data(args)\n",
    "        except ValueError as e:\n",
    "            print(\"Could not generate enough meaningful populations, \\\n",
    "                  increase number of attempts or consider changing min/max obs\")\n",
    "            continue\n",
    "        \n",
    "        _S = []\n",
    "        _I = []\n",
    "        _R = []\n",
    "        \n",
    "        s = args.population - 1\n",
    "        i = 1\n",
    "        r = 0\n",
    "\n",
    "        for t in range(args.duration):\n",
    "            s -= data['S2I'][t].item()\n",
    "            i += data['S2I'][t].item()\n",
    "            i -= data['I2R'][t].item()\n",
    "            r += data['I2R'][t].item()\n",
    "            _S.append(s)\n",
    "            _I.append(i)\n",
    "            _R.append(r)\n",
    "            \n",
    "        _S = torch.tensor(_S)/args.population\n",
    "        _I = torch.tensor(_I)/args.population\n",
    "        _R = torch.tensor(_R)/args.population\n",
    "        x_vals = np.arange(0, len(_S), 1)  # length of all arrays is the same as duration\n",
    "        # plot the threshold\n",
    "        ax[ii, j].plot([args.population/10]*len(_S), label='threshold', color='black', linestyle='dashed')\n",
    "        ax[ii, j].set_yscale('log')        \n",
    "        \n",
    "        ax[ii, j].plot(_S, label='S', color='yellow', alpha=0.9)\n",
    "        ax[ii, j].fill_between(x_vals, _S, color='yellow', alpha=0.15)\n",
    "        ax[ii, j].plot(_I, label='I', color='red', alpha=0.9)\n",
    "        ax[ii, j].fill_between(x_vals, _I, color='red', alpha=0.35)\n",
    "        ax[ii, j].plot(_R, label='R', color='green', alpha=0.9)\n",
    "        ax[ii, j].fill_between(x_vals, _R, color='green', alpha=0.45)\n",
    "        ax[ii, j].legend(loc=\"upper right\")\n",
    "        ax[ii, j].set_title(\"u = {:.2f}\".format(args.policy_intervention))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_S, _I, _R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# =========\n",
    "#\n",
    "# While the above discrete_model is easy to understand, its discrete latent\n",
    "# variables pose a challenge for inference. One of the most popular inference\n",
    "# strategies for such models is Sequential Monte Carlo. However since Pyro and\n",
    "# PyTorch are stronger in gradient based vectorizable inference algorithms, we\n",
    "# will instead pursue inference based on Hamiltonian Monte Carlo (HMC).\n",
    "#\n",
    "# Our general inference strategy will be to:\n",
    "# 1. Introduce auxiliary variables to make the model Markov.\n",
    "# 2. Introduce more auxiliary variables to create a discrete parameterization.\n",
    "# 3. Marginalize out all remaining discrete latent variables.\n",
    "# 4. Vectorize to enable parallel-scan temporal filtering.\n",
    "#\n",
    "# Let's consider reparameterizing in terms of the variables (S, I) rather than\n",
    "# (S2I, I2R). Since these may lead to inconsistent states, we need to replace\n",
    "# the Binomial transition factors (S2I, I2R) with ExtendedBinomial.\n",
    "#\n",
    "# The following model is equivalent to the discrete_model:\n",
    "\n",
    "@config_enumerate\n",
    "def reparameterized_discrete_model(args, data):\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population) # again, these must be guesses? They have to change somehow.. \n",
    "\n",
    "    # Sequentially sample time-local variables.\n",
    "    S_curr = torch.tensor(args.population - 1.)\n",
    "    I_curr = torch.tensor(1.)\n",
    "    for t, datum in enumerate(data):\n",
    "        # Sample reparameterizing variables.\n",
    "        # When reparameterizing to a factor graph, we ignored density via\n",
    "        # .mask(False). Thus distributions are used only for initialization.\n",
    "        S_prev, I_prev = S_curr, I_curr\n",
    "        S_curr = pyro.sample(\"S_{}\".format(t),\n",
    "                             dist.Binomial(args.population, 0.5).mask(False))\n",
    "        I_curr = pyro.sample(\"I_{}\".format(t),\n",
    "                             dist.Binomial(args.population, 0.5).mask(False))\n",
    "\n",
    "        # Now we reverse the computation.\n",
    "        S2I = S_prev - S_curr\n",
    "        I2R = I_prev - I_curr + S2I\n",
    "        pyro.sample(\"S2I_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()),\n",
    "                    obs=S2I)\n",
    "        pyro.sample(\"I2R_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(I_prev, prob_i),\n",
    "                    obs=I2R)\n",
    "        pyro.sample(\"obs_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S2I, rho),\n",
    "                    obs=datum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparametrization and Auxiliary Variables\n",
    "\n",
    "First, read the explanation for the reparametrized model offered in the comments above. What they imply is that in their setup they don't directly assume an SIR model in terms of the $S$ ->($\\beta$)-> $I$ and $I$ ->($\\gamma$)-> $R$ transitions since they use the parameters corresponding to reproduction number $R_0$ and $\\rho$ which are a more natural formulation that include quantities of interest for us. When we perform inference, however, it is much easier to work with the model if we can reparametrize it in terms of $\\beta$ (transition probability) and $\\gamma$ (recovery time) that we originally defined and discuss in the SIR model dynamics section above. Doing this additionally allows us to discretize the time steps and work with the same compartmental model and use real-world data to fit the reparametrized model (params $\\beta$, $\\gamma$) while still estimating the posterior distributions over $R_0$ and $\\rho$. They define `S`, `I`, and implicitly, `R` instead of having to work with `S2I` and `I2R`. This is how we define auxiliary variables in the reparametrized model.\n",
    "\n",
    "The benefits of reparametrizing models this way will become much clearer in the SEI3R model where have far more complicated model dynamics to work with that can be implemented by reparametrizing the model. For our priors over the model parameters, we rely on estimated confidence intervals obtained from the experiments in recent literature on COVID-19 modeling as specified in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By reparameterizing, we have converted to coordinates that make the model\n",
    "# Markov. We have also replaced dynamic integer_interval constraints with\n",
    "# easier static integer_interval constraints (although we'll still need good\n",
    "# initialization to avoid NANs). Since the discrete latent variables are\n",
    "# bounded (by population size), we can enumerate out discrete latent variables\n",
    "# and perform HMC inference over the global latents. However enumeration\n",
    "# complexity is O(population^4), so this is only feasible for very small\n",
    "# populations.\n",
    "#\n",
    "# Here is an inference approach using an MCMC sampler.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _infer_hmc(args, data, model, init_values={}):\n",
    "    \"\"\"\n",
    "    Run Hamiltonian Monte Carlo mcmc to estimate parameters R0 and rho\n",
    "    \"\"\"\n",
    "    logging.info(\"Running inference...\")\n",
    "    kernel = NUTS(model,\n",
    "                  full_mass=[(\"R0\", \"rho\")],\n",
    "                  max_tree_depth=args.max_tree_depth,\n",
    "                  init_strategy=init_to_value(values=init_values),\n",
    "                  jit_compile=args.jit, ignore_jit_warnings=True)\n",
    "\n",
    "    # We'll define a hook_fn to log potential energy values during inference.\n",
    "    # This is helpful to diagnose whether the chain is mixing.\n",
    "    energies = []\n",
    "\n",
    "    def hook_fn(kernel, *unused):\n",
    "        e = float(kernel._potential_energy_last)\n",
    "        energies.append(e)\n",
    "        if args.verbose:\n",
    "            logging.info(\"potential = {:0.6g}\".format(e))\n",
    "\n",
    "    mcmc = MCMC(kernel, hook_fn=hook_fn,\n",
    "                num_samples=args.num_samples,\n",
    "                warmup_steps=args.warmup_steps)\n",
    "    mcmc.run(args, data)\n",
    "    mcmc.summary()\n",
    "    if args.plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(energies)\n",
    "        plt.xlabel(\"MCMC step\")\n",
    "        plt.ylabel(\"potential energy\")\n",
    "        plt.title(\"MCMC energy trace\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    samples = mcmc.get_samples()\n",
    "    return samples\n",
    "\n",
    "def infer_hmc_enum(args, data):\n",
    "    model = reparameterized_discrete_model\n",
    "    return _infer_hmc(args, data, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: For our reference only\n",
    "Debugging models in Pyro is conventionally done by running them forward and obtaining the trace to sanity check values. Or at least that's one approach to debugging that allows us to inspect what is going on within the model. We can condition the model on the observed data and obtain the values of $R_0$ and $\\rho$ from the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noah and Swapneel's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging the model by running it forward and using the observed data to condition it\n",
    "debug_trace = poutine.trace(reparameterized_discrete_model).get_trace(args, x['obs'])\n",
    "debug_logp = debug_trace.log_prob_sum()\n",
    "debug_params = [debug_trace.nodes[name][\"value\"].unconstrained() for name in debug_trace.param_nodes]\n",
    "debug_trace.nodes['R0'][\"value\"], debug_trace.nodes['rho'][\"value\"]  # currently no inference is being performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_model(model, *args):\n",
    "    \"\"\"\n",
    "    Debugging the model by running it forward and using the observed data to condition it\n",
    "    \n",
    "    model specifies the model to be traced\n",
    "    *args specifies the arguments for the model\n",
    "    \n",
    "    returns a poutine.trace object\n",
    "    \n",
    "    >> tr_ = debug_model(reparameterized_discrete_model, args, x['obs'])\n",
    "    \"\"\"\n",
    "    debug_trace = poutine.trace(reparameterized_discrete_model).get_trace(*args)\n",
    "    debug_logp = debug_trace.log_prob_sum()\n",
    "    debug_params = [debug_trace.nodes[name][\"value\"].unconstrained() for name in debug_trace.param_nodes]\n",
    "    return debug_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_samples(varname, _trace):\n",
    "    \"\"\"\n",
    "    Helper function to visualize plots by extracting all samples of a latent variable (S, I, R).\n",
    "    Filters the nodes by name from trace dict and returns their values. \n",
    "    Note: Expects that the variable name is underscore-separated from numeric value e.e. S_0, S_1, ...\n",
    "    \n",
    "    >> x = generate_data(args)\n",
    "    >> debug_trace = poutine.trace(reparameterized_discrete_model).get_trace(args, x['obs'])\n",
    "    >> extract_samples('S', debug_trace)\n",
    "    \"\"\"\n",
    "    set_trace()\n",
    "    return [_trace.nodes[nodename][\"value\"] for nodename in _trace.nodes.keys() if varname==nodename.split('_')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pyro Official Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scale to large populations, we'll continue to reparameterize, this time\n",
    "# replacing each of (S_aux, I_aux) with a combination of a bounded real\n",
    "# variable and a Categorical variable with only four values.\n",
    "#\n",
    "# This is the crux: we can now perform HMC over the real variable and\n",
    "# marginalize out the Categorical variables using variable elimination.\n",
    "#\n",
    "# We first define a helper to create enumerated Categorical sites.\n",
    "\n",
    "def quantize(name, x_real, min, max):\n",
    "    \"\"\"\n",
    "    Randomly quantize in a way that preserves probability mass.\n",
    "    We use a piecewise polynomial spline of order 3.\n",
    "    \"\"\"\n",
    "    assert min < max\n",
    "    lb = x_real.detach().floor()\n",
    "\n",
    "    # This cubic spline interpolates over the nearest four integers, ensuring\n",
    "    # piecewise quadratic gradients.\n",
    "    s = x_real - lb     # s in [0, 1)\n",
    "    ss = s * s          # ss in [0, 1), ss < s\n",
    "    t = 1 - s           # t in [0, 1)\n",
    "    tt = t * t          # tt in [0, 1), tt < t\n",
    "    probs = torch.stack([\n",
    "        t * tt,\n",
    "        4 + ss * (3 * s - 6),\n",
    "        4 + tt * (3 * t - 6),\n",
    "        s * ss,\n",
    "    ], dim=-1) * (1/6)  # this sums probs to 1. so essentially drawing from a uniform over 0 to 3\n",
    "    q = pyro.sample(\"Q_\" + name, dist.Categorical(probs)).type_as(x_real)\n",
    "\n",
    "    x = lb + q - 1\n",
    "    x = torch.max(x, 2 * min - 1 - x)\n",
    "    x = torch.min(x, 2 * max + 1 - x)\n",
    "\n",
    "    return pyro.deterministic(name, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparametrizing the Model as Continuous\n",
    "\n",
    "There is another reparametrization that allows us to use a continuous model for generating the samples. However we have to quantize this to allow for enumeration over discrete latent variables during inference. Besides, people cannot be \"fractional\" so it makes sense to model it thus.\n",
    "\n",
    "For technical details, chain the custom `debug_model` function and `extract_data` function to obtain plots as we will do following the model definition.\n",
    "\n",
    "Comments: I am still unclear on the relevance of the cubic spline and piecewise quadratic gradients that we obtain for each discrete time-local variable. I think the idea here is that we want to use HMC which requires only continuous random variables in the model because it calculates the gradient of the distribution with respect to the parameters and runs MCMC until convergence to the stationary distribution. However, it cannot do so with discrete latent variables and so we have to reparametrize the model and enumerate over them. I'm not sure if this makes much sense because of jargon and my little procedural understanding of HMC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define another equivalent model.\n",
    "\n",
    "@config_enumerate\n",
    "def continuous_model(args, data):\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population)\n",
    "\n",
    "    # Sample reparameterizing variables.\n",
    "    S_aux = pyro.sample(\"S_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "    I_aux = pyro.sample(\"I_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "\n",
    "    # Sequentially sample time-local variables.\n",
    "    S_curr = torch.tensor(args.population - 1.)\n",
    "    I_curr = torch.tensor(1.)\n",
    "    for t, datum in poutine.markov(enumerate(data)):\n",
    "        S_prev, I_prev = S_curr, I_curr\n",
    "        S_curr = quantize(\"S_{}\".format(t), S_aux[..., t], min=0, max=args.population)\n",
    "        I_curr = quantize(\"I_{}\".format(t), I_aux[..., t], min=0, max=args.population)\n",
    "\n",
    "        # Now we reverse the computation.\n",
    "        S2I = S_prev - S_curr\n",
    "        I2R = I_prev - I_curr + S2I\n",
    "        pyro.sample(\"S2I_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()),\n",
    "                    obs=S2I)\n",
    "        pyro.sample(\"I2R_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(I_prev, prob_i),\n",
    "                    obs=I2R)\n",
    "        pyro.sample(\"obs_{}\".format(t),\n",
    "                    dist.ExtendedBinomial(S2I, rho),\n",
    "                    obs=datum)\n",
    "        set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now all latent variables in the continuous_model are either continuous or\n",
    "# enumerated, so we can use HMC. However we need to take special care with\n",
    "# constraints because the above Markov reparameterization covers regions of\n",
    "# hypothesis space that are infeasible (i.e. whose log_prob is -infinity). We\n",
    "# thus heuristically initialize to a feasible point.\n",
    "\n",
    "def heuristic_init(args, data):\n",
    "    \"\"\"Heuristically initialize to a feasible point.\"\"\"\n",
    "    # Start with a single infection.\n",
    "    S0 = args.population - 1\n",
    "    # Assume 50% <= response rate <= 100%.\n",
    "    S2I = data * min(2., (S0 / data.sum()).sqrt()) # smarter guess for ? \n",
    "    S_aux = (S0 - S2I.cumsum(-1)).clamp(min=0.5) # clamp \n",
    "    # Account for the single initial infection.\n",
    "    S2I[0] += 1\n",
    "    # Assume infection lasts less than a month.\n",
    "    recovery = torch.arange(30.).div(args.recovery_time).neg().exp()\n",
    "    I_aux = convolve(S2I, recovery)[:len(data)].clamp(min=0.5)\n",
    "\n",
    "    return {\n",
    "        \"R0\": torch.tensor(2.0),\n",
    "        \"rho\": torch.tensor(0.5),\n",
    "        \"S_aux\": S_aux,\n",
    "        \"I_aux\": I_aux,\n",
    "    }\n",
    "\n",
    "\n",
    "def infer_hmc_cont(model, args, data):\n",
    "    init_values = heuristic_init(args, data)\n",
    "    return _infer_hmc(args, data, model, init_values=init_values)\n",
    "\n",
    "\n",
    "# Our final inference trick is to vectorize. We can repurpose DiscreteHMM's\n",
    "# implementation here, but we'll need to manually represent a Markov\n",
    "# neighborhood of multiple Categorical of size 4 as single joint Categorical\n",
    "# with 4 * 4 = 16 states, and then manually perform variable elimination (the\n",
    "# factors here don't quite conform to DiscreteHMM's interface).\n",
    "\n",
    "def quantize_enumerate(x_real, min, max):\n",
    "    \"\"\"\n",
    "    Randomly quantize in a way that preserves probability mass.\n",
    "    We use a piecewise polynomial spline of order 3.\n",
    "    \"\"\"\n",
    "    assert min < max\n",
    "    lb = x_real.detach().floor()\n",
    "\n",
    "    # This cubic spline interpolates over the nearest four integers, ensuring\n",
    "    # piecewise quadratic gradients.\n",
    "    s = x_real - lb\n",
    "    ss = s * s\n",
    "    t = 1 - s\n",
    "    tt = t * t\n",
    "    probs = torch.stack([\n",
    "        t * tt,\n",
    "        4 + ss * (3 * s - 6),\n",
    "        4 + tt * (3 * t - 6),\n",
    "        s * ss,\n",
    "    ], dim=-1) * (1/6)\n",
    "    logits = safe_log(probs)\n",
    "    q = torch.arange(-1., 3.)\n",
    "\n",
    "    x = lb.unsqueeze(-1) + q\n",
    "    x = torch.max(x, 2 * min - 1 - x)\n",
    "    x = torch.min(x, 2 * max + 1 - x)\n",
    "    return x, logits\n",
    "\n",
    "\n",
    "def vectorized_model(args, data):\n",
    "    # Sample global parameters.\n",
    "    rate_s, prob_i, rho = global_model(args.population)\n",
    "\n",
    "    # Sample reparameterizing variables.\n",
    "    S_aux = pyro.sample(\"S_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "    I_aux = pyro.sample(\"I_aux\",\n",
    "                        dist.Uniform(-0.5, args.population + 0.5)\n",
    "                            .mask(False).expand(data.shape).to_event(1))\n",
    "\n",
    "    # Manually enumerate.\n",
    "    S_curr, S_logp = quantize_enumerate(S_aux, min=0, max=args.population)\n",
    "    I_curr, I_logp = quantize_enumerate(I_aux, min=0, max=args.population)\n",
    "    # Truncate final value from the right then pad initial value onto the left.\n",
    "    S_prev = torch.nn.functional.pad(S_curr[:-1], (0, 0, 1, 0), value=args.population - 1)\n",
    "    I_prev = torch.nn.functional.pad(I_curr[:-1], (0, 0, 1, 0), value=1)\n",
    "    # Reshape to support broadcasting, similar to EnumMessenger.\n",
    "    T = len(data)\n",
    "    Q = 4\n",
    "    S_prev = S_prev.reshape(T, Q, 1, 1, 1)\n",
    "    I_prev = I_prev.reshape(T, 1, Q, 1, 1)\n",
    "    S_curr = S_curr.reshape(T, 1, 1, Q, 1)\n",
    "    S_logp = S_logp.reshape(T, 1, 1, Q, 1)\n",
    "    I_curr = I_curr.reshape(T, 1, 1, 1, Q)\n",
    "    I_logp = I_logp.reshape(T, 1, 1, 1, Q)\n",
    "    data = data.reshape(T, 1, 1, 1, 1)\n",
    "\n",
    "    # Reverse the S2I,I2R computation.\n",
    "    S2I = S_prev - S_curr\n",
    "    I2R = I_prev - I_curr + S2I\n",
    "\n",
    "    # Compute probability factors.\n",
    "    S2I_logp = dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()).log_prob(S2I)\n",
    "    I2R_logp = dist.ExtendedBinomial(I_prev, prob_i).log_prob(I2R)\n",
    "    obs_logp = dist.ExtendedBinomial(S2I, rho).log_prob(data)\n",
    "\n",
    "    # Manually perform variable elimination.\n",
    "    logp = S_logp + (I_logp + obs_logp) + S2I_logp + I2R_logp\n",
    "    logp = logp.reshape(-1, Q * Q, Q * Q)\n",
    "    logp = pyro.distributions.hmm._sequential_logmatmulexp(logp)\n",
    "    logp = logp.reshape(-1).logsumexp(0)\n",
    "    logp = logp - math.log(4)  # Account for S,I initial distributions.\n",
    "    warn_if_nan(logp)\n",
    "    pyro.factor(\"obs\", logp)\n",
    "\n",
    "\n",
    "# We can fit vectorized_model exactly as we fit the original continuous_model,\n",
    "# using our infer_hmc_cont helper. The vectorized model is more than an order\n",
    "# of magnitude faster than the sequential version, and scales logarithmically\n",
    "# in time (up to your machine's parallelism).\n",
    "#\n",
    "# After inference we have samples of all latent variables. Let's define a\n",
    "# helper to examine the inferred posterior distributions.\n",
    "\n",
    "def evaluate(args, samples):\n",
    "    # Print estimated values.\n",
    "    names = {\"basic_reproduction_number\": \"R0\",\n",
    "             \"response_rate\": \"rho\"}\n",
    "    for name, key in names.items():\n",
    "        mean = samples[key].mean().item()\n",
    "        std = samples[key].std().item()\n",
    "        logging.info(\"{}: truth = {:0.3g}, estimate = {:0.3g} \\u00B1 {:0.3g}\"\n",
    "                     .format(key, getattr(args, name), mean, std))\n",
    "\n",
    "    # Optionally plot histograms.\n",
    "    if args.plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(5, 5))\n",
    "        axes[0].set_title(\"Posterior parameter estimates\")\n",
    "        for ax, (name, key) in zip(axes, names.items()):\n",
    "            truth = getattr(args, name)\n",
    "            sns.distplot(samples[key], ax=ax, label=\"posterior\")\n",
    "            ax.axvline(truth, color=\"k\", label=\"truth\")\n",
    "            ax.set_xlabel(key + \" = \" + name.replace(\"_\", \" \"))\n",
    "            ax.set_yticks(())\n",
    "            ax.legend(loc=\"best\")\n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction and Forecasting\n",
    "# ==========================\n",
    "#\n",
    "# So far we've written four models that each describe the same probability\n",
    "# distribution. Each successive model made inference cheaper. Next let's move\n",
    "# beyond inference and consider predicting latent infection rate and\n",
    "# forecasting future infections.\n",
    "#\n",
    "# We'll use Pyro's effect handlers to combine multiple of the above models,\n",
    "# leveraging the vectorized_model for inference, then the continuous_model to\n",
    "# compute local latent variables, and finally the original discrete_model to\n",
    "# forecast forward in time. Let's assume posterior samples have already been\n",
    "# generated via infer_hmc_cont(vectorized_model, ...).\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(args, data, samples, truth=None):\n",
    "    logging.info(\"Forecasting {} steps ahead...\".format(args.forecast))\n",
    "    particle_plate = pyro.plate(\"particles\", args.num_samples, dim=-1)\n",
    "\n",
    "    # First we sample discrete auxiliary variables from the continuous\n",
    "    # variables sampled in vectorized_model. This samples only time steps\n",
    "    # [0:duration]. Here infer_discrete runs a forward-filter backward-sample\n",
    "    # algorithm. We'll add these new samples to the existing dict of samples.\n",
    "    model = poutine.condition(continuous_model, samples)\n",
    "    model = particle_plate(model)\n",
    "    model = infer_discrete(model, first_available_dim=-2)\n",
    "    with poutine.trace() as tr:\n",
    "        model(args, data)\n",
    "    samples = OrderedDict((name, site[\"value\"])\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if site[\"type\"] == \"sample\")\n",
    "\n",
    "    # Next we'll run the forward generative process in discrete_model. This\n",
    "    # samples time steps [duration:duration+forecast]. Again we'll update the\n",
    "    # dict of samples.\n",
    "    extended_data = list(data) + [None] * args.forecast\n",
    "    model = poutine.condition(discrete_model, samples)\n",
    "    model = particle_plate(model)\n",
    "    with poutine.trace() as tr:\n",
    "        model(args, extended_data)\n",
    "    samples = OrderedDict((name, site[\"value\"])\n",
    "                          for name, site in tr.trace.nodes.items()\n",
    "                          if site[\"type\"] == \"sample\")\n",
    "\n",
    "    # Finally we'll concatenate the sequentially sampled values into contiguous\n",
    "    # tensors. This operates on the entire time interval [0:duration+forecast].\n",
    "    for key in (\"S\", \"I\", \"S2I\", \"I2R\"):\n",
    "        pattern = key + \"_[0-9]+\"\n",
    "        series = [value\n",
    "                  for name, value in samples.items()\n",
    "                  if re.match(pattern, name)]\n",
    "        assert len(series) == args.duration + args.forecast\n",
    "        series[0] = series[0].expand(series[1].shape)\n",
    "        samples[key] = torch.stack(series, dim=-1)\n",
    "    S2I = samples[\"S2I\"]\n",
    "    median = S2I.median(dim=0).values\n",
    "    logging.info(\"Median prediction of new infections (starting on day 0):\\n{}\"\n",
    "                 .format(\" \".join(map(str, map(int, median)))))\n",
    "\n",
    "    # Optionally plot the latent and forecasted series of new infections.\n",
    "    if args.plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        time = torch.arange(args.duration + args.forecast)\n",
    "        p05 = S2I.kthvalue(int(round(0.5 + 0.05 * args.num_samples)), dim=0).values\n",
    "        p95 = S2I.kthvalue(int(round(0.5 + 0.95 * args.num_samples)), dim=0).values\n",
    "        plt.fill_between(time, p05, p95, color=\"red\", alpha=0.3, label=\"90% CI\")\n",
    "        plt.plot(time, median, \"r-\", label=\"median\")\n",
    "        plt.plot(time[:args.duration], data, \"k.\", label=\"observed\")\n",
    "        if truth is not None:\n",
    "            plt.plot(time, truth, \"k--\", label=\"truth\")\n",
    "        plt.axvline(args.duration - 0.5, color=\"gray\", lw=1)\n",
    "        plt.xlim(0, len(time) - 1)\n",
    "        plt.ylim(0, None)\n",
    "        plt.xlabel(\"day after first infection\")\n",
    "        plt.ylabel(\"new infections per day\")\n",
    "        plt.title(\"New infections in population of {}\".format(args.population))\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(__debug__)\n",
    "pyro.set_rng_seed(args.rng_seed)\n",
    "\n",
    "\n",
    "dataset = generate_data(args)\n",
    "obs = dataset[\"obs\"][:args.duration]\n",
    "# Choose among inference methods.\n",
    "#if args.enum:\n",
    "#    samples = infer_hmc_enum(args, obs)\n",
    "#elif args.sequential:\n",
    "#    samples = infer_hmc_cont(continuous_model, args, obs)\n",
    "#else:\n",
    "samples = infer_hmc_cont(vectorized_model, args, obs)\n",
    "\n",
    "# Evaluate fit.\n",
    "evaluate(args, samples)\n",
    "\n",
    "# Predict latent time series.\n",
    "if args.forecast:\n",
    "    samples = predict(args, obs, samples, truth=dataset[\"S2I\"])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_data(args)\n",
    "obs = dataset[\"obs\"][:args.duration]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['S2I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['obs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = predict(args, obs, samples, truth=dataset[\"S2I\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges to consider\n",
    "\n",
    "1. What about having only data from timestep t to T (where t > 0). How do we condition on this data?\n",
    "[This paper](https://papers.nips.cc/paper/2016/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf) explains an idea to impute missing subsequences in a time series but that seems a computationally hard challenge that we do not need to solve right now. What about using FRED with different parameter settings to try and arrive at the best initializations for the generation of the observed data and then using those values to predict an initial time series. Well isn't that what pyro will do if we simply structure it such that obs is empty for the first few time steps? I think so, it's just that FRED offers more realistic cimulations so we should look into that part.\n",
    "\n",
    "Oh lol, we can just add [None] * t to the front of the observations for simulating a partially observed outbreak.\n",
    "\n",
    "Hm, but if you consider partial in the sense uncertainty in estimates of i(t) that is an interesting idea maybe we can modify and add noise to create simultaneous sets of observations and condition a separate model on each of these observation sets and run something like a grid-search to obtain confidence intervals for parameter estimates. Is that the best we can do? What about assuming independent Gaussian noise, is there a better idea?\n",
    "\n",
    "2. Add more latent variables to make it realistic, notably 'u', but that is already under consideration once we define an SEI3R model.\n",
    "\n",
    "3. Deaths need to be accounted for and maybe births can be added in? TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T16:06:18.663551Z",
     "start_time": "2020-12-10T16:06:18.655373Z"
    }
   },
   "outputs": [],
   "source": [
    "## Matplotlib Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T16:37:30.345Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T16:37:32.454Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [0, 0.5, 1, 1, 1.5, 1.5, 2, 2, 2, 2.5, 2.5, 2.5, 3, 3, 3.5, 3.5, 4, 4, 4.5, 5]\n",
    "y = [0, 1, 0, 2, 1, 3, 0, 2, 4, 1, 3, 0, 2, 4, 1, 3, 0, 2, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T16:37:51.380Z"
    }
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x[:10], y[:10], label=\"b=0.2\", color=\"red\")\n",
    "plt.scatter(x[10:], y[10:], label=\"b=0.8\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T16:38:26.074Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions as dist\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T16:40:59.223Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "probs = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "for i in range(n):\n",
    "    indices = np.random.permutation(8)\n",
    "    bern = dist.Bernoulli(probs[indices])\n",
    "    if i == 0:\n",
    "        samples = bern.sample(torch.Size([1000]))\n",
    "    else:\n",
    "        samples += bern.sample(torch.Size([1000]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T16:36:48.265208Z",
     "start_time": "2020-12-10T16:36:48.243252Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(data=samples, x=\"total_bill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "pyprobenv",
   "language": "python",
   "name": "pyprobenv"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
